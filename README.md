# Cargo-Data-ETL-Pipeline-using-AWS
An automated ETL pipeline built on AWS to ingest, transform, and analyze cargo data stored in Excel/CSV format. This project demonstrates a scalable, cost-optimized architecture using AWS Lambda, Glue, S3, and Redshift.

---

## ðŸš€ Tech Stack

- **AWS Lambda** â€“ Event-based ingestion from S3
- **AWS Glue** â€“ Data transformation and ETL
- **Amazon S3** â€“ Staging, processing, and archive layers
- **Amazon Redshift** â€“ Final storage and analytics
- **AWS SNS & CloudWatch** â€“ Alerting and monitoring
- **Parquet** â€“ Optimized storage format
- **Python** â€“ Lambda & Glue scripts

---

## âš™ï¸ Features

- âœ… Serverless ingestion using AWS Lambda
- âœ… Parquet-based storage for cost-efficient querying
- âœ… Automated lifecycle policies to reduce S3 costs
- âœ… End-to-end monitoring and alerting with CloudWatch & SNS
- âœ… Ready for scaling across large data volumes

---

## ðŸ“ Use Case

> A logistics company receives daily CSV files of cargo load data. This pipeline automates ingestion, transformation, storage, and analysis without any manual effort, reducing cost and improving reliability.

---

## ðŸ“„ License

This project is licensed under the [MIT License](LICENSE).





