# Cargo-Data-ETL-Pipeline-using-AWS
An automated ETL pipeline built on AWS to ingest, transform, and analyze cargo data stored in Excel/CSV format. This project demonstrates a scalable, cost-optimized architecture using AWS Lambda, Glue, S3, and Redshift.

---

## 🚀 Tech Stack

- **AWS Lambda** – Event-based ingestion from S3
- **AWS Glue** – Data transformation and ETL
- **Amazon S3** – Staging, processing, and archive layers
- **Amazon Redshift** – Final storage and analytics
- **AWS SNS & CloudWatch** – Alerting and monitoring
- **Parquet** – Optimized storage format
- **Python** – Lambda & Glue scripts

---

## ⚙️ Features

- ✅ Serverless ingestion using AWS Lambda
- ✅ Parquet-based storage for cost-efficient querying
- ✅ Automated lifecycle policies to reduce S3 costs
- ✅ End-to-end monitoring and alerting with CloudWatch & SNS
- ✅ Ready for scaling across large data volumes

---

## 📝 Use Case

> A logistics company receives daily CSV files of cargo load data. This pipeline automates ingestion, transformation, storage, and analysis without any manual effort, reducing cost and improving reliability.

---

## 📄 License

This project is licensed under the [MIT License](LICENSE).





